{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "110b5bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: rouge_score in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (0.29.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.7.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (19.0.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q nltk datasets transformers torch sentencepiece sentence_transformers\n",
    "!pip install evaluate rouge rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b373a15e-a766-480f-b171-7caed31fa280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy/__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy/cli/_util.py\", line 68, in setup_cli\n",
      "    registry.cli.get_all()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/catalogue/__init__.py\", line 110, in get_all\n",
      "    result.update(self.get_entry_points())\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/catalogue/__init__.py\", line 125, in get_entry_points\n",
      "    result[entry_point.name] = entry_point.load()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/importlib/metadata/__init__.py\", line 171, in load\n",
      "    module = import_module(match.group('module'))\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy_curated_transformers/cli/debug_pieces.py\", line 20, in <module>\n",
      "    from ..pipeline.transformer import CuratedTransformer\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy_curated_transformers/pipeline/__init__.py\", line 1, in <module>\n",
      "    from .transformer import CuratedTransformer\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy_curated_transformers/pipeline/transformer.py\", line 24, in <module>\n",
      "    from ..models.listeners import ListenerStateUtils\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy_curated_transformers/models/__init__.py\", line 1, in <module>\n",
      "    from .architectures import (\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/spacy_curated_transformers/models/architectures.py\", line 17, in <module>\n",
      "    from thinc.api import (\n",
      "ImportError: cannot import name 'TorchScriptWrapper_v1' from 'thinc.api' (/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/thinc/api.py)\n",
      "Requirement already satisfied: cupy-cuda12x in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (13.3.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cupy-cuda12x) (1.26.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cupy-cuda12x) (0.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets nltk spacy\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a127eae-8215-438d-ae63-3de966d66dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator12recordStreamERKNS_7DataPtrENS0_10CUDAStreamE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from datasets import load_dataset, DatasetDict,Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# from evaluate import load\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "# rouge = load(\"rouge\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a039cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41cf9af-6f73-4ca5-82b9-51f7081d1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_summary_text(text):\n",
    "    \"\"\"\n",
    "    Pre-clean the summary text to fix artifacts such as a conjunction\n",
    "    immediately followed by a period. For example, \"John and . Audrey\" becomes\n",
    "    \"John and Audrey\".\n",
    "    \"\"\"\n",
    "    # Remove a period if it follows a conjunction with only whitespace in between.\n",
    "    text = re.sub(r'\\b(and|or|but)\\s*\\.\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0507d6f-3148-4f00-828c-d79bc4218f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_length(text):\n",
    "    \"\"\"\n",
    "    Dummy token count: assume each word is a token.\n",
    "    \"\"\"\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e202af22-1956-435e-9755-2362562b5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_incomplete_sentences(sentences, min_tokens=10):\n",
    "    \"\"\"\n",
    "    Merge summary sentences that appear incomplete.\n",
    "    \n",
    "    Heuristic:\n",
    "      - If a sentence has fewer than `min_tokens` tokens,\n",
    "      - Or if it ends with a conjunction (e.g. \"and\", \"or\", \"but\") possibly\n",
    "        followed by punctuation,\n",
    "    then remove trailing punctuation and merge it with the following sentence.\n",
    "    \n",
    "    Returns a new list of sentences.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        current = sentences[i].strip()\n",
    "        # Remove extra punctuation at the end if it looks like an artifact.\n",
    "        current = re.sub(r'\\s*[,\\.;:]+\\s*$', '', current)\n",
    "        tokens = current.split()\n",
    "        # If not the last sentence and the sentence is very short or ends with a conjunction...\n",
    "        if i < len(sentences) - 1 and (len(tokens) < min_tokens or re.search(r'\\b(and|or|but)$', current, re.IGNORECASE)):\n",
    "            # Merge current with the next sentence (and remove any extra punctuation).\n",
    "            next_sent = re.sub(r'^\\s*[,\\.;:]+\\s*', '', sentences[i+1].strip())\n",
    "            merged.append(current + \" \" + next_sent)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(current)\n",
    "            i += 1\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60fcd99-5ead-46dd-bade-6431e1da0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_person_name(name):\n",
    "    \"\"\"\n",
    "    Normalize a person name so that variants map to the same key.\n",
    "    For example, \"Harry Potter\" and \"H. Potter\" will both be converted to \"h_potter\".\n",
    "    \n",
    "    The heuristic:\n",
    "      - Remove punctuation,\n",
    "      - Lowercase the text,\n",
    "      - Split into tokens,\n",
    "      - If two or more tokens exist, return: first letter of the first token + '_' + last token.\n",
    "      - Otherwise, return the token.\n",
    "    \"\"\"\n",
    "    name_clean = re.sub(r'[^\\w\\s]', '', name).strip().lower()\n",
    "    tokens = name_clean.split()\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0]\n",
    "    return tokens[0][0] + \"_\" + tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca6b3d6-90fc-4036-b844-037197da041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def anonymize_names_in_text(text, mapping, next_person_index):\n",
    "    \"\"\"\n",
    "    Replace person names in the text with placeholders like \"person_1\".\n",
    "    \n",
    "    Uses spaCy to detect PERSON entities. Contiguous PERSON entities (e.g.,\n",
    "    \"Albert and Connon\") are merged into one entity. Each unique normalized person\n",
    "    name is replaced with a placeholder.\n",
    "    \n",
    "    Parameters:\n",
    "      - text: The input text.\n",
    "      - mapping: A dict mapping normalized names to placeholders.\n",
    "      - next_person_index: The next available placeholder index.\n",
    "    \n",
    "    Returns:\n",
    "      - anonymized_text: The text after replacement.\n",
    "      - mapping: The updated mapping.\n",
    "      - next_person_index: The updated next available index.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    person_entities = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            person_entities.append((ent.start_char, ent.end_char, ent.text))\n",
    "    \n",
    "    # Merge contiguous PERSON entities.\n",
    "    merged_entities = []\n",
    "    if person_entities:\n",
    "        current_start, current_end, current_text = person_entities[0]\n",
    "        for start, end, ent_text in person_entities[1:]:\n",
    "            gap = text[current_end:start]\n",
    "            if re.match(r'^\\s*(and|,|&)?\\s*$', gap):\n",
    "                current_end = end\n",
    "                current_text += \" \" + ent_text\n",
    "            else:\n",
    "                merged_entities.append((current_start, current_end, current_text))\n",
    "                current_start, current_end, current_text = start, end, ent_text\n",
    "        merged_entities.append((current_start, current_end, current_text))\n",
    "    \n",
    "    anonymized_text = \"\"\n",
    "    last_index = 0\n",
    "    for start, end, name in merged_entities:\n",
    "        anonymized_text += text[last_index:start]\n",
    "        norm = normalize_person_name(name)\n",
    "        if norm not in mapping:\n",
    "            mapping[norm] = f\"person_{next_person_index}\"\n",
    "            next_person_index += 1\n",
    "        anonymized_text += mapping[norm]\n",
    "        last_index = end\n",
    "    anonymized_text += text[last_index:]\n",
    "    return anonymized_text, mapping, next_person_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "717fb976-ee74-4db7-9c35-399301d84386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_identical_article_chunks(examples):\n",
    "    \"\"\"\n",
    "    Merge examples that have identical article chunks by concatenating their summary sentences.\n",
    "    \n",
    "    Parameters:\n",
    "      - examples: A list of dictionaries with keys \"article\", \"highlight\", and \"id\".\n",
    "      \n",
    "    Returns:\n",
    "      - A new list of examples where examples with identical article chunks have been merged.\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    for ex in examples:\n",
    "        article = ex[\"article\"]\n",
    "        if article not in merged:\n",
    "            merged[article] = {\n",
    "                \"article\": article,\n",
    "                \"highlights\": ex[\"highlights\"],\n",
    "                \"id\": ex[\"id\"]\n",
    "            }\n",
    "        else:\n",
    "            merged[article][\"highlights\"] += \" \" + ex[\"highlights\"]\n",
    "    return list(merged.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf0045a-9801-4a34-a9eb-1558a06fa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_tfidf(example, sim_threshold=0.3, max_total_tokens=400):\n",
    "    \"\"\"\n",
    "    Process one input row (a dict with keys \"article\" and \"highlights\")\n",
    "    and output one or more rows mapping each summary sentence to a matching\n",
    "    article chunk using TF-IDF cosine similarity.\n",
    "    \n",
    "    For each summary sentence:\n",
    "      1. Identify the best matching article sentence (A) via cosine similarity.\n",
    "      2. Append the following sentence (B), if available. If (A+B) exceeds the allowed token count,\n",
    "         then truncate the end of B so that A+B fits exactly.\n",
    "      3. If (A+B) is shorter than the allowed token count and a sentence before A (C) exists,\n",
    "         prepend C. If adding C makes the candidate exceed the limit, then truncate tokens from the beginning of C.\n",
    "      4. Discard the pair if the resulting candidate article chunk or summary is empty.\n",
    "    \n",
    "    Parameters:\n",
    "      - example (dict): Must contain keys \"article\" and \"highlights\".\n",
    "      - sim_threshold (float): (Not used in candidate selection here.)\n",
    "      - max_total_tokens (int): Maximum allowed token (word) count for article + summary.\n",
    "    \n",
    "    Returns:\n",
    "      - List of dictionaries, each with keys \"article_chunk\" and \"summary_chunk\".\n",
    "        Pairs with empty chunks are discarded.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    # Retrieve text from the example.\n",
    "    article_text = example.get(\"article\", \"\")\n",
    "    summary_text = example.get(\"highlights\", \"\")\n",
    "    \n",
    "    # Sentence tokenization.\n",
    "    article_sents = sent_tokenize(article_text)\n",
    "    summary_sents = sent_tokenize(summary_text)\n",
    "    \n",
    "    if not article_sents or not summary_sents:\n",
    "        return []\n",
    "    \n",
    "    # Precompute tokenized versions and lengths for article sentences.\n",
    "    article_tokens = [sent.split() for sent in article_sents]\n",
    "    article_token_lens = [len(tokens) for tokens in article_tokens]\n",
    "    \n",
    "    # Precompute tokenized versions for summary sentences.\n",
    "    summary_tokens_list = [sent.split() for sent in summary_sents]\n",
    "    \n",
    "    # Fit the TF-IDF vectorizer on the article sentences.\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(article_sents)\n",
    "    \n",
    "    # Compute TF-IDF vectors for article sentences.\n",
    "    article_vectors = vectorizer.transform(article_sents)\n",
    "    \n",
    "    # Batch transform all summary sentences.\n",
    "    summary_vectors = vectorizer.transform(summary_sents)\n",
    "    # Compute similarity matrix: each row corresponds to a summary sentence.\n",
    "    similarity_matrix = cosine_similarity(summary_vectors, article_vectors)\n",
    "    \n",
    "    output_rows = []\n",
    "    for i, summ in enumerate(summary_sents):\n",
    "        # Skip empty summary.\n",
    "        if not summ.strip():\n",
    "            continue\n",
    "\n",
    "        summ_tokens = summary_tokens_list[i]\n",
    "        len_summ = len(summ_tokens)\n",
    "        allowed_tokens = max_total_tokens - len_summ\n",
    "        if allowed_tokens <= 0:\n",
    "            continue  # Cannot form candidate if summary already exceeds max_total_tokens.\n",
    "        \n",
    "        # Step 1: Find best matching article sentence A.\n",
    "        sims = similarity_matrix[i]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        A = article_sents[best_idx]\n",
    "        tokens_A = article_tokens[best_idx]\n",
    "        \n",
    "        # Initialize candidate with A.\n",
    "        candidate_tokens = tokens_A[:]\n",
    "        candidate_text = A\n",
    "        \n",
    "        # Step 2: Append following sentence B if available.\n",
    "        if best_idx < len(article_sents) - 1:\n",
    "            B = article_sents[best_idx + 1]\n",
    "            tokens_B = article_tokens[best_idx + 1]\n",
    "            # Check if full A+B fits.\n",
    "            if len(candidate_tokens) + len(tokens_B) <= allowed_tokens:\n",
    "                candidate_tokens += tokens_B\n",
    "                candidate_text = A + \" \" + B\n",
    "            else:\n",
    "                # Only add as many tokens from B as possible.\n",
    "                allowed_for_B = allowed_tokens - len(candidate_tokens)\n",
    "                if allowed_for_B > 0:\n",
    "                    truncated_B_tokens = tokens_B[:allowed_for_B]\n",
    "                    candidate_tokens += truncated_B_tokens\n",
    "                    candidate_text = A + \" \" + \" \".join(truncated_B_tokens)\n",
    "                # Else, no tokens from B can be added.\n",
    "        \n",
    "        # Step 3: If candidate is still shorter than allowed and there's a preceding sentence C.\n",
    "        if len(candidate_tokens) < allowed_tokens and best_idx > 0:\n",
    "            C = article_sents[best_idx - 1]\n",
    "            tokens_C = article_tokens[best_idx - 1]\n",
    "            # Prepend entire C.\n",
    "            candidate_with_C_tokens = tokens_C + candidate_tokens\n",
    "            if len(candidate_with_C_tokens) <= allowed_tokens:\n",
    "                candidate_tokens = candidate_with_C_tokens\n",
    "                candidate_text = C + \" \" + candidate_text\n",
    "            else:\n",
    "                # Determine how many tokens from C can be added.\n",
    "                allowed_for_C = allowed_tokens - len(candidate_tokens)\n",
    "                if allowed_for_C > 0:\n",
    "                    # Truncate C from its beginning (i.e. drop the earliest tokens)\n",
    "                    truncated_C_tokens = tokens_C[-allowed_for_C:]\n",
    "                    candidate_tokens = truncated_C_tokens + candidate_tokens\n",
    "                    candidate_text = \" \".join(truncated_C_tokens) + \" \" + candidate_text\n",
    "                # Else: cannot add any tokens from C.\n",
    "        \n",
    "        # Final check: if candidate or summary is empty, skip this pair.\n",
    "        if not candidate_text.strip() or not summ.strip():\n",
    "            continue\n",
    "        \n",
    "        output_rows.append({\n",
    "            \"article_chunk\": candidate_text,\n",
    "            \"summary_chunk\": summ\n",
    "        })\n",
    "    \n",
    "    return output_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a631448-0380-4a7b-ad04-6c71d38afff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 287113/287113 [27:30<00:00, 174.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "# Process each example with a tqdm progress bar.\n",
    "processed_examples = []\n",
    "# subset = dataset_train.select(range(100))\n",
    "for example in tqdm(dataset_train, desc=\"Processing examples\"):\n",
    "    # Each example may generate one or more output rows.\n",
    "    new_rows = process_row_tfidf(example, sim_threshold=0.2, max_total_tokens=400)\n",
    "    processed_examples.extend(new_rows)\n",
    "\n",
    "# Create a new Hugging Face dataset from the processed examples.\n",
    "new_dataset = Dataset.from_list(processed_examples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "201d154e-1653-4e47-91e7-0875918c4f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a757ce8915c14a41be237fc35f8a33d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1057334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dataset.save_to_disk(\"chunked_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a79592-58b2-41a9-878d-65d506cff065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_chunk': 'The four soldiers face minimum mandatory sentences of 10 years in prison each for the drug conspiracy and an additional five years, consecutive, for the weapons allegation. The Army Rangers are an elite light infantry fighting force capable of deploying anywhere in the world within 18 hours. They became a permanent presence in the U.S. military in the 1970s.',\n",
       " 'summary_chunk': 'The Army Rangers are an elite light infantry fighting force .'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b24db5-9a5b-4a71-88ed-8ca539513820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 11490/11490 [00:44<00:00, 258.32it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7e896103144c92b48888e9ad5304a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/43560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "# Process each example with a tqdm progress bar.\n",
    "processed_examples = []\n",
    "for example in tqdm(dataset_train, desc=\"Processing examples\"):\n",
    "    # Each example may generate one or more output rows.\n",
    "    new_rows = process_row_tfidf(example, sim_threshold=0.2, max_total_tokens=400)\n",
    "    processed_examples.extend(new_rows)\n",
    "\n",
    "# Create a new Hugging Face dataset from the processed examples.\n",
    "new_dataset = Dataset.from_list(processed_examples)\n",
    "new_dataset.save_to_disk(\"chunked_test\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715a581b-96b9-42ce-87f5-02efa7d225c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 13368/13368 [00:51<00:00, 258.52it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f275642ddf64d31b074dd8479e732df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/53480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation\")\n",
    "# Process each example with a tqdm progress bar.\n",
    "processed_examples = []\n",
    "for example in tqdm(dataset_train, desc=\"Processing examples\"):\n",
    "    # Each example may generate one or more output rows.\n",
    "    new_rows = process_row_tfidf(example, sim_threshold=0.2, max_total_tokens=400)\n",
    "    processed_examples.extend(new_rows)\n",
    "\n",
    "# Create a new Hugging Face dataset from the processed examples.\n",
    "new_dataset = Dataset.from_list(processed_examples)\n",
    "new_dataset.save_to_disk(\"chunked_validation\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d2bfa-c767-4bde-82b0-bdd9450a8c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
